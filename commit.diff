diff --git a/./scripts/pytorch_main.py b/changed.py
index d6b0b35..586947a 100644
--- a/./scripts/pytorch_main.py
+++ b/changed.py
@@ -20,6 +20,15 @@ import torchvision.transforms as transforms
 import torchvision.datasets as datasets
 import torchvision.models as models
 
+import sys
+import torchvision
+from torch.utils.data import Dataset
+from PIL import Image
+from torch.utils.tensorboard import SummaryWriter
+import numpy as np
+
+writer = SummaryWriter('./run')
+
 model_names = sorted(name for name in models.__dict__
     if name.islower() and not name.startswith("__")
     and callable(models.__dict__[name]))
@@ -76,6 +85,12 @@ parser.add_argument('--multiprocessing-distributed', action='store_true',
                          'fastest way to use PyTorch for either single node or '
                          'multi node data parallel training')
 
+#######
+parser.add_argument('--result_name', default='val_result', type=str,
+                    help='[WANGRIRUI] filename of npy which save the validation result.')
+#######
+
+
 best_acc1 = 0
 
 
@@ -136,7 +151,7 @@ def main_worker(gpu, ngpus_per_node, args):
         model = models.__dict__[args.arch](pretrained=True)
     else:
         print("=> creating model '{}'".format(args.arch))
-        model = models.__dict__[args.arch]()
+        model = models.__dict__[args.arch](num_classes=200)
 
     if not torch.cuda.is_available():
         print('using CPU, this will be slow')
@@ -214,7 +229,6 @@ def main_worker(gpu, ngpus_per_node, args):
         traindir,
         transforms.Compose([
             transforms.RandomResizedCrop(224),
-            transforms.RandomHorizontalFlip(),
             transforms.ToTensor(),
             normalize,
         ]))
@@ -227,14 +241,14 @@ def main_worker(gpu, ngpus_per_node, args):
     train_loader = torch.utils.data.DataLoader(
         train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),
         num_workers=args.workers, pin_memory=True, sampler=train_sampler)
-
-    val_loader = torch.utils.data.DataLoader(
-        datasets.ImageFolder(valdir, transforms.Compose([
-            transforms.Resize(256),
-            transforms.CenterCrop(224),
-            transforms.ToTensor(),
-            normalize,
-        ])),
+    
+    val_dataset = Val_Dataset(valdir,transforms.Compose([
+                    transforms.Resize(256),
+                    transforms.CenterCrop(224),
+                    transforms.ToTensor(),
+                    normalize,
+                    ]))
+    val_loader = torch.utils.data.DataLoader(val_dataset,
         batch_size=args.batch_size, shuffle=False,
         num_workers=args.workers, pin_memory=True)
 
@@ -286,25 +300,40 @@ def train(train_loader, model, criterion, optimizer, epoch, args):
     model.train()
 
     end = time.time()
-    for i, (images, target) in enumerate(train_loacondader):
+    for i, (images, target) in enumerate(train_loader):
         # measure data loading time
         data_time.update(time.time() - end)
-
+        ######
+        #print('training on %dth image'%(i+1))
+        #sys.stdout.flush()
+        ######
         if args.gpu is not None:
             images = images.cuda(args.gpu, non_blocking=True)
         if torch.cuda.is_available():
             target = target.cuda(args.gpu, non_blocking=True)
-
+        
+        
+        
         # compute output
         output = model(images)
         loss = criterion(output, target)
-
+        
+        ######
+        torch.cuda.empty_cache()
+        ######
+        
         # measure accuracy and record loss
         acc1, acc5 = accuracy(output, target, topk=(1, 5))
         losses.update(loss.item(), images.size(0))
         top1.update(acc1[0], images.size(0))
         top5.update(acc5[0], images.size(0))
-
+        
+        ######
+        writer.add_scalar('Loss/train', loss, i)
+        writer.add_scalar('Acc@1/train', acc1, i)
+        writer.add_scalar('Acc@5/train', acc5, i)
+        ######
+        
         # compute gradient and do SGD step
         optimizer.zero_grad()
         loss.backward()
@@ -330,7 +359,12 @@ def validate(val_loader, model, criterion, args):
 
     # switch to evaluate mode
     model.eval()
-
+    
+    
+    ######
+    the_result = np.array([])
+    ######
+    
     with torch.no_grad():
         end = time.time()
         for i, (images, target) in enumerate(val_loader):
@@ -349,13 +383,32 @@ def validate(val_loader, model, criterion, args):
             top1.update(acc1[0], images.size(0))
             top5.update(acc5[0], images.size(0))
 
+            
+            ######
+            if args.evaluate:
+                writer.add_images('image_batch', images, i)
+                val_prob = output.data.cpu().numpy()
+                val_result = np.argmax(val_prob, axis=1)
+                the_result = np.concatenate((the_result, val_result))    
+            else:
+                writer.add_scalar('Loss/Val', loss, i)
+                writer.add_scalar('Acc@1/train', acc1, i)
+                writer.add_scalar('Acc@5/train', acc5, i)
+            ######
+            
             # measure elapsed time
             batch_time.update(time.time() - end)
             end = time.time()
 
             if i % args.print_freq == 0:
                 progress.display(i)
-
+        
+        ######
+        if args.evaluate:
+            np.save(args.result_name + '.npy', the_result)
+            print('the length of result array is %d'%the_result.shape[0])
+        ######
+        
         progress.display_summary()
 
     return top1.avg
@@ -449,6 +502,42 @@ def accuracy(output, target, topk=(1,)):
             res.append(correct_k.mul_(100.0 / batch_size))
         return res
 
-
+class Val_Dataset(Dataset):
+    def __init__(self, valdir, transform):
+        self.valdir = valdir
+        self.transform = transform
+        
+        val_anno = os.path.join(self.valdir, 'val_annotations.txt')
+        with open(val_anno, 'r') as fr:
+            lines = fr.readlines()
+            classes = [line.split('\t')[1] for line in lines]
+            self.classes = sorted(list(set(classes)))
+            self.img_name = [line.split('\t')[0] for line in lines]
+            # 得到图像名到类的对应关系
+            self.img_to_classes = {self.img_name[i]:classes[i] for i in range(len(lines))}
+            self.size = len(lines) 
+        # 得到类到标签的关系(经验证后与训练集保持一致)
+        self.class_to_target = {self.classes[i]:i for i in range(len(self.classes))}
+        
+        self.images = []
+        root = os.path.join(self.valdir, 'images')
+        for name in self.img_name:
+            path = os.path.join(root, name)
+            target = self.class_to_target[self.img_to_classes[name]]
+            self.images.append((path, target))
+    
+    def __len__(self):
+        return self.size
+    
+    def __getitem__(self, idx):
+        path, target = self.images[idx]
+        img = Image.open(path)
+        img = img.convert('RGB')
+        if self.transform is not None:
+            img = self.transform(img) 
+        return img, target
+    
+    
+    
 if __name__ == '__main__':
     main()
\ No newline at end of file
diff --git a/./scripts/pytorch_main.py b/changed.py
index 30fd04c..71fd435 100644
--- a/./scripts/pytorch_main.py
+++ b/changed.py
@@ -20,6 +20,15 @@ import torchvision.transforms as transforms
 import torchvision.datasets as datasets
 import torchvision.models as models
 
+import sys
+import torchvision
+from torch.utils.data import Dataset
+from PIL import Image
+from torch.utils.tensorboard import SummaryWriter
+import numpy as np
+
+writer = SummaryWriter('./run')
+
 model_names = sorted(name for name in models.__dict__
     if name.islower() and not name.startswith("__")
     and callable(models.__dict__[name]))
@@ -76,6 +85,12 @@ parser.add_argument('--multiprocessing-distributed', action='store_true',
                          'fastest way to use PyTorch for either single node or '
                          'multi node data parallel training')
 
+#######
+parser.add_argument('--result_name', default='val_result', type=str,
+                    help='[WANGRIRUI] filename of npy which save the validation result.')
+#######
+
+
 best_acc1 = 0
 
 
@@ -136,7 +151,7 @@ def main_worker(gpu, ngpus_per_node, args):
         model = models.__dict__[args.arch](pretrained=True)
     else:
         print("=> creating model '{}'".format(args.arch))
-        model = models.__dict__[args.arch]()
+        model = models.__dict__[args.arch](num_classes=200)
 
     if not torch.cuda.is_available():
         print('using CPU, this will be slow')
@@ -214,7 +229,6 @@ def main_worker(gpu, ngpus_per_node, args):
         traindir,
         transforms.Compose([
             transforms.RandomResizedCrop(224),
-            transforms.RandomHorizontalFlip(),
             transforms.ToTensor(),
             normalize,
         ]))
@@ -227,14 +241,14 @@ def main_worker(gpu, ngpus_per_node, args):
     train_loader = torch.utils.data.DataLoader(
         train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),
         num_workers=args.workers, pin_memory=True, sampler=train_sampler)
-
-    val_loader = torch.utils.data.DataLoader(
-        datasets.ImageFolder(valdir, transforms.Compose([
-            transforms.Resize(256),
-            transforms.CenterCrop(224),
-            transforms.ToTensor(),
-            normalize,
-        ])),
+    
+    val_dataset = Val_Dataset(valdir,transforms.Compose([
+                    transforms.Resize(256),
+                    transforms.CenterCrop(224),
+                    transforms.ToTensor(),
+                    normalize,
+                    ]))
+    val_loader = torch.utils.data.DataLoader(val_dataset,
         batch_size=args.batch_size, shuffle=False,
         num_workers=args.workers, pin_memory=True)
 
@@ -294,17 +308,29 @@ def train(train_loader, model, criterion, optimizer, epoch, args):
             images = images.cuda(args.gpu, non_blocking=True)
         if torch.cuda.is_available():
             target = target.cuda(args.gpu, non_blocking=True)
-
+        
+        
+        
         # compute output
         output = model(images)
         loss = criterion(output, target)
-
+        
+        ######
+        torch.cuda.empty_cache()
+        ######
+        
         # measure accuracy and record loss
         acc1, acc5 = accuracy(output, target, topk=(1, 5))
         losses.update(loss.item(), images.size(0))
         top1.update(acc1[0], images.size(0))
         top5.update(acc5[0], images.size(0))
-
+        
+        ######
+        writer.add_scalar('Loss/train', loss, i)
+        writer.add_scalar('Acc@1/train', acc1, i)
+        writer.add_scalar('Acc@5/train', acc5, i)
+        ######
+        
         # compute gradient and do SGD step
         optimizer.zero_grad()
         loss.backward()
@@ -330,7 +356,12 @@ def validate(val_loader, model, criterion, args):
 
     # switch to evaluate mode
     model.eval()
-
+    
+    
+    ######
+    the_result = np.array([])
+    ######
+    
     with torch.no_grad():
         end = time.time()
         for i, (images, target) in enumerate(val_loader):
@@ -349,13 +380,32 @@ def validate(val_loader, model, criterion, args):
             top1.update(acc1[0], images.size(0))
             top5.update(acc5[0], images.size(0))
 
+            
+            ######
+            if args.evaluate:
+                writer.add_images('image_batch', images, i)
+                val_prob = output.data.cpu().numpy()
+                val_result = np.argmax(val_prob, axis=1)
+                the_result = np.concatenate((the_result, val_result))    
+            else:
+                writer.add_scalar('Loss/Val', loss, i)
+                writer.add_scalar('Acc@1/train', acc1, i)
+                writer.add_scalar('Acc@5/train', acc5, i)
+            ######
+            
             # measure elapsed time
             batch_time.update(time.time() - end)
             end = time.time()
 
             if i % args.print_freq == 0:
                 progress.display(i)
-
+        
+        ######
+        if args.evaluate:
+            np.save(args.result_name + '.npy', the_result)
+            print('the length of result array is %d'%the_result.shape[0])
+        ######
+        
         progress.display_summary()
 
     return top1.avg
@@ -449,6 +499,42 @@ def accuracy(output, target, topk=(1,)):
             res.append(correct_k.mul_(100.0 / batch_size))
         return res
 
-
+class Val_Dataset(Dataset):
+    def __init__(self, valdir, transform):
+        self.valdir = valdir
+        self.transform = transform
+        
+        val_anno = os.path.join(self.valdir, 'val_annotations.txt')
+        with open(val_anno, 'r') as fr:
+            lines = fr.readlines()
+            classes = [line.split('\t')[1] for line in lines]
+            self.classes = sorted(list(set(classes)))
+            self.img_name = [line.split('\t')[0] for line in lines]
+            # 得到图像名到类的对应关系
+            self.img_to_classes = {self.img_name[i]:classes[i] for i in range(len(lines))}
+            self.size = len(lines) 
+        # 得到类到标签的关系(经验证后与训练集保持一致)
+        self.class_to_target = {self.classes[i]:i for i in range(len(self.classes))}
+        
+        self.images = []
+        root = os.path.join(self.valdir, 'images')
+        for name in self.img_name:
+            path = os.path.join(root, name)
+            target = self.class_to_target[self.img_to_classes[name]]
+            self.images.append((path, target))
+    
+    def __len__(self):
+        return self.size
+    
+    def __getitem__(self, idx):
+        path, target = self.images[idx]
+        img = Image.open(path)
+        img = img.convert('RGB')
+        if self.transform is not None:
+            img = self.transform(img) 
+        return img, target
+    
+    
+    
 if __name__ == '__main__':
     main()
\ No newline at end of file
